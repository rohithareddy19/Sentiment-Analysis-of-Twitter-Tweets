{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extracting the tweets from twitter using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input conditions\n",
    "query = input('Enter query (use %23 for hashtags, use %20 for space, use %3 for account): ')\n",
    "option = input('Do you want to filter by date Y/N: ')\n",
    "if option == \"Y\":\n",
    "    start = input('enter start date(YYYY-MM-DD): ')\n",
    "    end = input('enter the end date(YYYY-MM-DD): ')\n",
    "    filter_by_date = '%20since%3A'+start+'%20until%3A'+end\n",
    "    browser = webdriver.Chrome(r'C:\\Users\\Reddy Rohitha\\chromedriver.exe')\n",
    "    base_url = u'https://twitter.com/search?f=tweets&vertical=default&q='\n",
    "    url = base_url + query + filter_by_date\n",
    "else:\n",
    "    browser = webdriver.Chrome(r'C:\\Users\\Reddy Rohitha\\chromedriver.exe')\n",
    "    base_url = u'https://twitter.com/search?f=tweets&vertical=default&q='\n",
    "    #base_url = u'https://twitter.com/search?q='\n",
    "    url = base_url + query\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to scroll the page\n",
    "def tweet_scroller(url):\n",
    "\n",
    "    browser.get(url)\n",
    "    \n",
    "    #define initial page height for 'while' loop\n",
    "    lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #define how many seconds to wait while dynamic page content loads\n",
    "        time.sleep(3)\n",
    "        newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        else:\n",
    "            lastHeight = newHeight\n",
    "            \n",
    "    html = browser.page_source\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling beautiful soup to get the tweets\n",
    "soup = BeautifulSoup(tweet_scroller(url), \"html.parser\")\n",
    "\n",
    "all_tweets = soup.find_all('div',{'class':'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the respective attributes\n",
    "one_list = []\n",
    "headers_list = [\"User\",\"Tweet_id\",\"Timestamp\",\"Message\",\"Hashtags\",\"Replies\",\"Retweets\",\"Likes\"]\n",
    "final_list =[]\n",
    "final_list.append(headers_list)\n",
    "for tweet in all_tweets:\n",
    "    #context = tweet.find('div',{'class':'context'}).text.replace(\"\\n\",\" \").strip()\n",
    "    content = tweet.find('div',{'class':'content'})\n",
    "    header = content.find('div',{'class':'stream-item-header'})\n",
    "    footer = content.find('div',{'class':'stream-item-footer'})\n",
    "    user = re.findall(\"@\\w+\",header.find('a',{'class':'account-group js-account-group js-action-profile js-user-profile-link js-nav'}).text.replace(\"\\n\",\" \").strip())[0]\n",
    "    time = header.find('a',{'class':'tweet-timestamp js-permalink js-nav js-tooltip'}).prettify()#.find('span').text.replace(\"\\n\",\" \").strip()\n",
    "    index1 = time.find('data-conversation-id')\n",
    "    index2 = time.find('href')\n",
    "    tweet_id= time[index1+22:index2-2]\n",
    "    index3 = time.find('title')\n",
    "    index4 = time.find('span')\n",
    "    timestamp = time[index3+7:index4-5]\n",
    "    message = content.find('div',{'class':'js-tweet-text-container'}).text.replace(\"\\n\",\" \").strip()\n",
    "    hashtags = \" \".join(re.findall(\"#[a-zA-Z]+\", message))\n",
    "    replies = footer.find_all('span',{'class':'ProfileTweet-actionCountForPresentation'})[0].get_text()\n",
    "    retweets = footer.find_all('span',{'class':'ProfileTweet-actionCountForPresentation'})[1].get_text()\n",
    "    likes = footer.find_all('span',{'class':'ProfileTweet-actionCountForPresentation'})[3].get_text()\n",
    "\n",
    "    one_list.append(user)\n",
    "    one_list.append(tweet_id)\n",
    "    one_list.append(timestamp)\n",
    "    one_list.append(message)\n",
    "    one_list.append(hashtags)\n",
    "    one_list.append(replies)\n",
    "    one_list.append(retweets)\n",
    "    one_list.append(likes)\n",
    "    final_list.append(one_list)\n",
    "    one_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use text formatting to save the file based on query \n",
    "path = r'C:\\Users\\Reddy Rohitha\\Documents\\BA CAPSTONE\\TWEETS\\tweets_%s.csv' %query\n",
    "\n",
    "with open(path, \"a\", newline='',encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',',lineterminator='\\n',quotechar='\"')\n",
    "    writer.writerows(final_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6cad6a69f057dfb3716d38f36f95c9fdd4eb0aeda346613983bded13370c981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
